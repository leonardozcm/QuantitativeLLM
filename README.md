# QuantitativeLLM
We optimize most of the operators used in LLM from scratch from the perspective of quantitative analysis of computer architecture. The programming language, framework, and even the final performance are not important. The most important thing is the thinking process of quantitative analysis.

## Experiment Device Parameters

### RTX2070s
```
Device count 1
Device name NVIDIA GeForce RTX 2070 SUPER
Amount of global memory: 7.60376 GB
Amount of total memory: 7.60376 GB avail memory: 7.50232 GB
Global memory bus width in bits:   256 bit
Compute capability:     7.5
Amount of constant memory:      64 KB
Maximum grid size:  2147483647 65535 65535
maximum block size:     1024 1024 64
Number of SMs:      40
Number of warpSize:      32
L2 Cache size:                             4096 KB
maximum l2 persisting lines capacity       0 B
Device supports caching globals in L1(Y/N) 1
Device supports caching locals in L1(Y/N)  1
Maximum amount of shared memory per block: 48 KB
Maximum amount of shared memory per SM:    64 KB
Maximum number of registers per block:     64 K
Maximum number of registers per SM:        64 K
Maximum number of threads per block:       1024
Maximum number of threads per SM:          1024
```